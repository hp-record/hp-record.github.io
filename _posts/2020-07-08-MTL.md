---
title: MTL
layout: post
categories: MTL
tags: 综述
excerpt: 大致归纳理解
---

---------

# 目录 <span id="home">

* **[1. 引言](#1)**
* **[2. 主要内容](#2)**
  * **[2.1 Multi-task Learning Network Design](#2.1)**
  * **[2.2 Multi-task Learning Loss Function Design ](#2.2)**
  * **[2.3 ](#2.3)**
* **[3. 总结](#3)**
* **[4. 参考列表](#4)**

---------

# 1. 引言 <span id="1">  

------

在绝大部分情况下，MTL（Multi Task Learning ） 的研究可以归类为以下两个方向，一个是 MTL Network 网络设计；另一个是 MTL Loss function 损失函数设计。

# 2. 主要内容<span id="2">  

## 2.1 Multi-task Learning Network Design / What to share? [网络设计]<span id='2.1'>

在起初，MTL 的网络设计通常可以列为两种情况：Hard parameter sharing 和 soft parameter sharing。

![多任务学习-软参数共享.png](https://i.loli.net/2020/07/08/ey6ir3jZqsQxWMI.png)

<center>图-多任务学习软参数共享</center>

![多任务学习-硬参数共享.png](https://i.loli.net/2020/07/08/qptsVUf2BH6Myb9.png)

<center>图-多任务学习硬参数共享</center>

硬参数共享（Hard-parameter sharing） -- 现在是几乎所有做 MTL 不可缺少的 baseline 之一，就是将整个 backbone 网络作为 shared network 来 encode 任务信息，在最后一层网络 split 成几个 task-specific decoders 做 prediction。Hard-parameter sharing 是网络设计参数数量 parameter space 的 (并不严格的，假设不考虑用 network pruning) lower bound，由此作为判断新设计网络对于 efficiency v.s. accuracy 平衡的重要参考对象。

软参数共享（Soft-parameter sharing ）-- 可以看做是 hard-parameter sharing 的另外一个极端，并不常见于现在 MTL 网络设计的比较。在 soft-parameter sharing 中，每一个任务都有其相同大小的 backbone network 作为 parameter space。我们对于其 parameter space 加于特定的 constraint 可以是 sparsity, 或 gradient similarity, 或 LASSO penalty 来 softly* constrain 不同任务网络的 representation space。假设我们不对于 parameter space 加以任何 constraint，那么 soft-parameter sharing 将塌缩成 single task learning。

任一 MTL 网络设计可以看做是找 hard 和 soft parameter sharing 的平衡点：1. 如何网络设计可以小巧轻便。2. 如何网络设计可以最大幅度的让不同任务去共享信息。

MTL network design is all about sharing.

**· Cross-Stitch Network**

Cross-Stitch Network 是过去几年内比较经典的网络设计，也已常用于各类 MTL 研究的baseline 之一。其核心思想是将每个独立的 task-specific 网络使用 learnable parameters (cross-stitch units) 以 linear combination 的方式连接其中不同任务的 convolutional blocks。

![Cross-Stitch Network](https://imgkr.cn-bj.ufileos.com/2bbbea8d-f048-4dd2-a080-75b8b7eef2f6.png)

Visualisation of Cross-Stitch Network

对于任务 A 与 B，每个 convolutional block 输出层 $$x_{A,B}^{i,j}$$，我们将计算：

$$\begin{bmatrix}\widetilde x_{A}^{i,j}\\\\\widetilde x_{B}^{i,j}\end{bmatrix}=\begin{bmatrix}\Lambda_{AA} &\Lambda_{AB} \\\\\Lambda_{BA}&\Lambda_{BB}\end{bmatrix}$$

通过这样的运算，下一个 convolutional block 的输入层则为 $$\widetilde x_{A,B}^{i,j}$$.

启发于 Cross-stitch 的设计，NDDR-CNN 也有类似的思路。然而不同的是，对于中间层的 convolutional block 的信息融合，他们采用了 concatenate 并通过 [1 x 1] 的 convolutional layer 来 reduce dimensionality。这样的设计使得每个任务的 channel 都可以与其他不同 index 的 channel 交融信息，而规避了原始 Cross-stitch 只能 infuse 相同 channel 信息的局限性。当 NDDR 的 convolutional layer weights 的 non-diagonal elements 是 0 的话， NDDR-CNN 则数学上等价于 Cross-Stich Network。

Cross-Stitch Network 和 NDDR-CNN 的最大弱势就是对于每个任务都需要一个新的网络，以此整个 parameter space 会对于任务的数量增加而线性增加，因此并不 efficient。

**· Multi-task Attention Network**

基于 Cross-stitch Network efficiency 的缺点，后续提出了 Multi-task Attention Network (MTAN) 让网络设计更加小巧轻便，整个网络的 parameter space 对于任务数量的增加以 sub-linearly 的方式增加。

![Multi-task Attention Network](https://imgkr.cn-bj.ufileos.com/cf2dd47d-6487-47d6-9405-b383e7024ac4.png)

MTAN 的核心思想是，assume 在 shared network encode 得到 general representation 信息之后，我们只需要少量的参数来 refine task-shared representation into task-specific representation, 就可以对于任意任务得到一个很好的 representation. 因此整个网络只需要增加少量的 task-specific attention module，两层 [1 x 1] conv layer，作为额外的 parameter space 来 attend 到 task-shared represenation。整个模型的参数相对于 Cross-Stitch Network 来说则大量减少。

**· AdaShare**
AdaShare 则更是将 MTL 网络设计的 efficiency 做到的极致。与其增加额外的 conv layer 来 refine task-shared representation，AdaShare 将单个 backbone 网络看做 representation 的整体，通过 differentiable task-specific policy 来决定对于任何一个 task，否用去更新或者利用这个网络的 block 的 representation。

![AdaShare](https://imgkr.cn-bj.ufileos.com/d6d266d5-5b60-4d1c-8b74-b595659e88bc.png)

由于整个网络是应用于所有任务的 representation，因此 network parameter space 是 agnostic 于任务数量，永远为常数，等价于 hard-parameter sharing。而搭接的 task-specific policy 是利用 gumbel-softmax 对于每一个 conv block 来 categorical sampling "select" 或者 "skip" 两种 policy，因为整个 MTL 的网络设计也因此会随着任务的不同而变化，类似于最近大火的 Neural Architecture Search 的思想。

- **MTL + NAS**

MTL-NAS 则是将 MTL 和 NAS 结合的另外一个例子。他搭载于 NDDR 的核心思想，将其拓展到任意 block 的交融，因此网络搜索于如何将不同 task 的不同 block 交融来获得最好的 performance。

![MTL-NAS](https://imgkr.cn-bj.ufileos.com/f630e1b9-b521-4677-8757-02076ed41daa.png)

个人更偏向 Adashare 的搜索方式，在单个网络里逐层搜索，这样的 task-specific representation 已经足够好过将每一个 task 定义成新网络的结果。由此， MTL-NAS 也躲不掉网络参数线性增加的特点，不过对于 MTL 网络设计提供了新思路。

MTL + NAS 和传统的 single-task NAS 会有着不同需求，和训练方式。

1. MTL+NAS 并不适合用 NAS 里最常见的 two-stage training 方式：以 validation performance 作为 supervision 来 update architecture 参数，得到 converged architecture 后再 re-train 整个网络。因为 MTL 的交融信息具备 training-adaptive 的性质， 因此 fix 网络结构后，这样的 training-adaptive 信息会丢失，得到的 performance 会低于边搜边收敛的 one-stage 方式。换句话说，训练中的 oscillation 和 feature fusion 对于 MTL 网络是更重要的，而在 single task learning 中，并没有 feature fusion 这个概念。这间接导致了 NAS 训练方式的需求不同。
2. MTL+NAS is task-specific. 在 NAS 训练中，要是 dataset 的 complexity 过大，有时候我们会采用 proxy task 的方式来加快训练速度。最常见的情况则是用 CIFAR-10 作为 proxy dataset 来搜好的网络结构，应用于过大的 ImageNet dataset。而这一方式并不适用于 MTL，因为对于任一任务，或者几个任务的 pair，他们所需要的 feature 信息和任务特性并不同，因此无法通过 proxy task 的方式来加速训练。每一组任务的网络都是独特和唯一的。



我相信在未来 MTL 网络设计的研究中，我们会得到更具备 interpretable/human-understandable 的网络特性，能够理解任务与任务之间的相关性，和复杂性。再通过得到的任务相关性，我们可以作为一个很好 prior knowledge 去 initialise 一个更好的起始网络，而由此得到一个更优秀的模型，一种良性循环。

A Better Task Relationship $$\Leftrightarrow$$ A Better Multi-task Architecture 

## 2.2 Multi-task Learning Loss Function Design / How to learn? [损失函数设计与梯度优化]<span id='2.2'>

平行于网络设计，另外一个较为热门的方向是 MTL 的 loss function design, 或者理解为如何去更好得 update 网络里的 task-specific gradients。

对于任意 task i, 我们有损失函数:$$L=\sum_i\alpha_iL_i$$ , 其中$$\alpha_i$$为 task-specific learning parameters. 那么，我们需要找到一组很好的$$\alpha_i$$来 optimise 所有 task i 的 performance $$L_i$$。其中最为简单且直接的方式则为 equal weighting: $$\alpha_i=1$$, 也就是默认每一个 task 对于 representation 的 contribution 是相同的。

- **Weight Uncertainty**

Weight Uncertainty 是最早几篇研究 MTL loss function design 的文章之一。这篇文章 assume 在每个 model 里存在一种 data-agnostic task-dependent uncertainty 称之为 Homoscedastic uncertainty。（这种说法其实非常的古怪，只有剑桥组喜欢这么称呼。）而通过 maximise log -likelihood of the model prediction uncertainty 可以来 balance MTL training。这里的 likelihood (通常 parameterised by Gaussian) 可以看做是 relative confidence between tasks。

对于任何 model prediction y, 我们定义 Gaussian likelihood of model: $$p(y|f_w(x))=N(f_w(x),\delta^2)$$其中这里的$$\delta$$为 learnable noise scalar (Gaussian variance)，那么我们需要 maximise:

$$logp(y|f_w(x))\propto-\frac{1}{2\delta^2}||y-f_w(x)||^2-log\delta$$

由此我们可以得到新定义的 loss function：

$$L=\sum_i\frac{1}{2\delta_i^2}L_i+log\delta_i$$

最后推导的公式非常简洁，也因此用在很多 MTL benchmark 里。

然而这篇文章有着非常大的争议，其中最著名的一点是作者对于如此简单的公式却一直拒绝开源，并且无视大量其他 researchers 的邮件对于 implementation 的细节询问，惹怒了不少同行（包括我）。此外，weight uncertainty 非常依赖于 optimiser 的选择，在我个人实验里，我发现有且仅有 ADAM optimiser 可以让 weight uncertainty 正确收敛，而在其他 optimiser 上 weight uncertainty 没有任何收敛趋势。这篇博客[1]则更为指出，这个 weight uncertainty 公式可以直接得到 closed-form solution：当 learnable $$\delta$$ is minimised, 整个 loss function 将转化成 geometric mean of task losses，因此再次对于这里 uncertainty assumption 可行性提出了质疑。

- **GradNorm**

GradNorm 则为另外一篇最早期做 MTL loss function 的文章。GradNorm 的实现是通过计算 inverse training rate: $$\widetilde L(t)=L(t)/L(0)$$下降速率，作为 indicator 来平衡不同任务之间的梯度更新。

我们定义 $$G^w(t)$$为 W 参数在 weighted multi-task loss 在 t step 上计算到梯度的 L2-norm; mean task gradient 为 $$\widetilde G^w(t)=E[G^w_i(t)]$$; relative inverse training rate 为 $$r_i(t)=\widetilde L_i(t)/E[\widetilde L_i(t)]$$。GradNorm 通过以下 objective 来更新 task-specific weighting:

$$|G^W_i(t)-\widetilde G^W_i(t)·r_i(t)^\alpha|$$

其中$$\widetilde G^W_i(t)·r_i(t)^\alpha$$则为理想的梯度 L2-norm (作为 constant), 来调整 task-specific weighting. $$\alpha$$作为一个平衡超参，$$\alpha$$ 越大则 task-specific weighting 越平衡。由于每次计算$$G^W(T)$$ 需要对所有 task 在每个 layer 进行 backprop，因此非常 computational expensive。由此，作者就以计算最后一层的 shared layer 作为 approximation 来加快训练速度。

- **Dynamic Weight Average**

由于 GradNorm 在计算 task-specific weighting 上需要运算两次 backprop 因此在 implementation 上非常复杂。我后续提出了一个简单的方法，只通过计算 loss 的 relative descending rate 来计算 task weighting:

$$\alpha_k(t):=\frac{Kexp(w_k(t-1)/T)}{\sum_iexp(w_k(t-1)/T},w_k(t-1)=\frac{\iota_k(t-1)}{\iota_k(t-2)}$$

这里的 则通过计算两个相邻的 time step 的 loss ratio 作为 descending rate。因此 越小，收敛速率就越大，任务就越简单，得到权重也就越小。

- **MTL as Multi-objective Optimisation**

之前介绍的几个 task-weighting 方法都基于一些特定的 heuristic，很难保证在 MTL optimisation 取得 optimum. 在 MTL as Multi-objective Optimisation 里，作者将 MTL 问题看做是多目标优化问题，其目标为取得 Pareto optimum.

Pareto optimum 是指任何对其中一个任务的 performance 变好的情况，一定会对其他剩余所有任务的 performance 变差。作者利用了一个叫 multiple gradient descent algorithm (MGDA) 的方法来寻找这个 Pareto stationary point。大致方式是，在每次计算 task-specific gradients 后，其得到 common direction 来更新 shared parameter s。这个 common direction 如果存在，则整个 optimisation 并未收敛到 Pareto optimum。这样的收敛方法保证了 shared parameter 不会出现 conflicting gradients 让每一个任务的 loss 收敛更加平滑。

- **Other heuristic methods**

对于 MTL loss function 的设计，还有其他不同的 heuristic 方法，或基于任务的难易性 (Dynamic Task Prioritization), 或直接对于计算到的任务梯度进行映射，防止出现任务梯度之间的 conflicting gradients 情况 (Gradient Surgery)。

但在各式各样的 MTL loss function design 里，很难出现其中一个方法在所有数据集里都 outperform 其他方法的情况。甚至，在部分数据集里，最简单的 equal task weighting 也表现得较为优异。一方面，task weighting 的有效性非常依赖于 MTL 网络本身的设计；此外 task weighting 的更新也依赖于数据集和 optimiser 本身。假设如果核心目标仅仅是取得最好的 MTL performance，那我建议应该花更多的时间去研究更好的网络而不是 task weighting。但不可否认的是，task weighting 的研究可以更好的帮助人们理解任务之间的相关性和复杂性，以此反过来帮助人们更好的设计模型本身。

# 3. 总结 <span id="3">  

---

近些年来 MTL 的研究出现了很多新颖且有价值的工作，但是对于任务自身的理解，和任务之间关系的理解还是有很大的不足和进步空间。在 Taskonomy 里，作者尝试了上千种（大量 CO2 排放）任务的组合来绘制出不同任务之间的关系图。但是真实 MTL 训练中，我相信这种关系图应该随着时间的变化而变化，且依赖网络本身。因此，如何更好得通过任务之间的关系去优化网络结构还是一个未解之谜，如何设计/生成辅助任务并通过 MTL 更好得帮助 primary task 也并未了解透彻。希望在后续的研究中能看到更多文章对于 MTL 的深入探索，实现 universal representation 的最终愿景。

# 4. 参考列表 <span id="4">  

---

[多任务学习: 过去，现在与未来](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247497004&idx=2&sn=ee2a3f138d1d289d8758c079e945aa3a&chksm=ec1c18d5db6b91c35938cd622324b447f0ac75d40661d339c003479fdbc0b057e8e112f330f4&mpshare=1&scene=1&srcid=&sharer_sharetime=1591179932775&sharer_shareid=989b0bb833dbad1aaaaf36960593e33d&key=d43cb7737f40d4df0b8a01ffbe987a84fda374d987b2dedcb6fe573013dcf125e141a5a95d397c6dcdfc9136625544b09609e3d105f1a8327ec301aa3a1a9ffc6e49fa3c4e667e88974a2950f7861547&ascene=1&uin=MTc5MjQxMzEyOQ%3D%3D&devicetype=Windows+10+x64&version=62090070&lang=zh_CN&exportkey=AwAv4N4Tbvx5DyATjJP8lXY%3D&pass_ticket=UE04Cv%2BEOVJKXVLGILtbq%2BDGVy9HnHR33uzPMkwnU8lcWwh9hSOig8Ttbu%2FIktXT)



